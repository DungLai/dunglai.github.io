<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      K-Means Clustering with Visualization tool &middot; Dung Lai
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110006044-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110006044-1');
  </script> 

  <!-- toogle button -->
  <script>
  function myFunction() {
      var x = document.getElementById("myDIV");
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
  }
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A personal website of <a href="https://dunglai.github.io/about" target="">Dung Lai</a>, containing technical blog on technology.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    
      
    
      
        
      
    
      
        
      
    
      
    
    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2018. All rights reserved.
    </p>
  </div>
</div>

 
    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Dung Lai</a>
            <small>blog</small>             
          </h3>
        </div>
      </div>
      
      <link rel="stylesheet" href="/public/css/blog-index.css">

<nav>
	<div class='blog-wrapper'>
		<div class='blog-index'>
			<strong><u>Machine learning</u></strong></br>
				<li><a href='https://dunglai.github.io/2018/01/25/SVD/'>7. Singular Value Decomposition</a></li>
				<li><a href='https://dunglai.github.io/2017/12/21/gradient-descent/'>6. Gradient Descent</a></li>
				<li><a href='https://dunglai.github.io/2017/12/09/k-nearest-neighbors/'>5. K Nearest Neighbors</a></li>
				<li><a href='https://dunglai.github.io/2017/10/10/linear-regression/'>4. Linear Regression</a></li>
				<li><a href='https://dunglai.github.io/2017/09/21/FlappyBirdAI/'>3. AI for flappy bird game (Neural Network, Genetic Algorithm)</a></li>
				<li><a href='https://dunglai.github.io/2017/06/10/image-compression/'>2. Image Segmentation using K-Means</a></li>
				<li><a href='https://dunglai.github.io/2017/06/01/k-means/'>1. K-Means Clustering</a></li>
			<strong><u>Other</u></strong></br>
				<li><a href='https://dunglai.github.io/2017/06/30/mongodbmapping/'>4. Mapping MySQL to MongoDB</a></li>
				<li><a href='https://dunglai.github.io/2017/06/27/UniversityMySQL/'>3. University MySQL DB</a></li>
				<li><a href='https://dunglai.github.io/2017/05/27/Planetary-Rover/'>2. Planetary Rover Game (C#, Winform)</a></li>
				<li><a href='https://dunglai.github.io/2017/03/01/Music-Player/'>1. Music Player (Pascal)</a></li>
		</div>
	</div>
</nav>

      <div class="container content">
        <div class="post">
  <h1 style="font-size: 130%;" class="post-title">K-Means Clustering with Visualization tool</h1>
  <span class="post-date" style="float:none;">01 Jun 2017</span>
  
  <div class="tag">Pascal</div>
<div class="tag">Machine Learning</div>
<p><strong>Content:</strong>
<!-- MarkdownTOC depth=3 --></p>

<ul>
  <li><a href="#1-objective">1. Objective</a></li>
  <li><a href="#2-introduction">2. Introduction</a></li>
  <li><a href="#3-mathematical-analysis">3. Mathematical Analysis</a>
    <ul>
      <li><a href="#31-input-and-output">3.1. Input and Output</a></li>
      <li><a href="#32-lost-function-and-optimization-problem">3.2. Lost Function and Optimization Problem</a></li>
      <li><a href="#33-solving-optimization-problem">3.3. Solving Optimization Problem</a>
        <ul>
          <li><a href="#331-fixed-%5Cmathbfm-center-of-observation-group">3.3.1. Fixed <script type="math/tex">\mathbf{M}</script>, center of observation group</a></li>
          <li><a href="#332-fixed-%5Cmathbfy-label-vector-of-each-observation">3.3.2. Fixed <script type="math/tex">\mathbf{Y}</script>, label vector of each observation</a></li>
        </ul>
      </li>
      <li><a href="#34-algorithm-summary-and-flowchart">3.4. Algorithm Summary and Flowchart</a>
        <ul>
          <li><a href="#341-summary">3.4.1. Summary</a></li>
          <li><a href="#342-flowchart">3.4.2. Flowchart</a></li>
        </ul>
      </li>
      <li><a href="#35-discussion">3.5. Discussion</a>
        <ul>
          <li><a href="#351-convergence">3.5.1. Convergence</a></li>
          <li><a href="#352-sensitiveness-to-initial-cluster">3.5.2. Sensitiveness to initial cluster</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#4-application-in-data-compression">4. Application in Data Compression</a></li>
  <li><a href="#5-conclusion">5. Conclusion</a></li>
  <li><a href="#6-acknowledgement">6. Acknowledgement</a>
    <ul>
      <li><a href="#61-reference">6.1. Reference</a></li>
    </ul>
  </li>
  <li><a href="#7-source-code-and-pdf-report">7. Source Code and PDF report</a>
    <ul>
      <li><a href="#71-declare-data-types">7.1. Declare Data types</a></li>
    </ul>
  </li>
</ul>

<!-- /MarkdownTOC -->
<p><a name="1-objective"></a></p>
<h2 id="1-objective">1. Objective</h2>
<p>After reading this blog, you will be able to</p>
<ol>
  <li>Understand a clustering method (unsupervised learning) namely K-means algorithm from mathematical perspective.</li>
  <li><strong>Source code</strong> of visualization tool (written in <strong>Pascal</strong>), demo below.</li>
  <li><strong>Source code</strong> of image compression, image segmentation tool, applied K-Means Algorithm (written in <strong>Pascal</strong>).</li>
</ol>
<embed class="video" width="1600" height="400" style="width: 100%" src="/public/post-assets/Kmeans/demo.mp4" scale="aspect" controller="true" />

<div class="thecap">Demo video</div>
<h5 id="11-abstract">1.1. <strong><em>Abstract</em></strong></h5>
<p>Clustering problem is a task of dividing a set of objects (also called members) into different groups (called clusters) based on object’s characteristics. Members of a group will have more similarities in comparison with those in other group. This report discusses a traditional clustering method called K-Means algorithm from mathematical perspective. Additionally, an experiment is provided to examine the algorithm in two dimensional space then an application in image compressing.
<a name="2-introduction"></a></p>
<h2 id="2-introduction">2. Introduction</h2>
<p>Clustering problems arise in many different applications: machine learning data mining and knowledge discovery, data compression and vector quantization, pattern recognition and pattern classification.</p>

<p>The goal of K-Means Algorithm is to correctly separating objects in a dataset into groups based on object’s properties. For instance, objects could be house and their properties are size, number of floor, location, power consumption per year, etc. The goal is to classify house dataset into groups which are luxury, average, poor. In that case, all properties of houses have to be processed to turn into number to create a vector, this process is called vectorization.</p>

<p>Another example, take each points in a panel as a objects and each object has two properties which are x-axis and y-axis location, with input <script type="math/tex">K=3</script>. The algorithm correctly finds the cluster (<a href="#fig1">fig 1</a>).</p>
<div class="imgcap">
<img style="float:left; display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/fig1.PNG" width="500" align="center" />
<img id="fig1" style="float:left; display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/fig2.PNG" width="500" align="center" />
<div class="thecap">Fig 1: Running K-means algorithm to find 3 cluster<br /></div>
</div>
<div style="clear:left;"></div>
<p><a name="3-mathematical-analysis"></a></p>
<h2 id="3-mathematical-analysis">3. Mathematical Analysis</h2>
<p><a name="31-input-and-output"></a></p>
<h3 id="31-input-and-output">3.1. Input and Output</h3>
<p>The K-Means Algorithm takes a set of observations <script type="math/tex">X=[x_1,x_2,...,x_N]</script> <script type="math/tex">\in</script> <script type="math/tex">R^{d \times N}</script> where each observation is a <script type="math/tex">d</script>-dimensional vector, <script type="math/tex">N</script> is the number of observations (members) and the number of group <script type="math/tex">% <![CDATA[
(K, K<N) %]]></script> as two input. The algorithm outputs the center of <script type="math/tex">K</script> group <script type="math/tex">[m_1,m_2,...,m_K] \in R^{d \times K}</script> and the index or name of group that each member belonged to (label of the group).
<a name="32-lost-function-and-optimization-problem"></a></p>
<h3 id="32-lost-function-and-optimization-problem">3.2. Lost Function and Optimization Problem</h3>
<p>Suppose <script type="math/tex">x_i</script> <script type="math/tex">(i\in[1,N])</script> belong to cluster <script type="math/tex">k</script> <script type="math/tex">(k \in [1,K]</script>, the lost value of observation <script type="math/tex">x_i</script> is the distance from observation <script type="math/tex">x_i</script> to center <script type="math/tex">m_k</script> in euclidean space, defined by <script type="math/tex">(x_i-m_k)</script>.
Let’s <script type="math/tex">y_i=[y_{i1},y_{i2},...,y_{iK}]</script> be the label vector of each observation <script type="math/tex">x_i</script>, <script type="math/tex">y_{ik}=1</script> if <script type="math/tex">x_i</script> belongs to group <script type="math/tex">k</script> and <script type="math/tex">y_{ij}=0</script> <script type="math/tex">\forall j\neq k</script>.
Label vector of each observation contains only one digit <script type="math/tex">1</script> because each observation belongs to only one group which leads to the following equation.
\begin{equation} \tag{1}\label{eq:1}
\sum_{k=1}^{K} y_{ik} = 1
\end{equation}
The objective is to minimize the within-cluster sum of squares (variance), also known as square errors of, where each square error of an observation <script type="math/tex">x_i</script> from group <script type="math/tex">m_k</script> is defined by:
\begin{equation} \tag{2}\label{eq:2}
||x_{i}-m_{k}||^2=y_{ik}||x_{i}-{m_k}||^2
\end{equation}
From the equation \eqref{eq:1}, sum of all elements in a label vector is equal to 1. The square error of an observation is:
\begin{equation} \tag{3}\label{eq:3}
y_{ik}||x_{i}-{m_k}||^2=\sum_{j=1}^{K}y_{ij}||x_{i}-m_{j}||^2
\end{equation}
The square error of all observation is the sum of every square error of in the given set of observation. The goal is minimize the lost function, equation \eqref{eq:4} where <script type="math/tex">Y=[y_1,y_2,...,y_N]</script> be the matrix contains all label vector of <script type="math/tex">N</script> observation and <script type="math/tex">M=[m_1,m_2,...,m_K]</script> be the center of <script type="math/tex">K</script> groups (clusters).
\begin{equation} \tag{4}\label{eq:4}
f(Y,M)=\sum_{i=1}^{N}\sum_{j=1}^{K}y_{ij}||x_i-m_j||^2
\end{equation}
The objective is also to find the center and label vector of each observation which are Y and M, the two outputs that are mentioned in <a href="#input-and-output">input and output</a>.
\begin{equation} \tag{5}\label{eq:5}
Y,M=argmin_{Y,M}\sum_{i=1}^{N}\sum_{j=1}^{K}y_{ij}||x_i-m_j||^2 
\end{equation}
<a name="33-solving-optimization-problem"></a></p>
<h3 id="33-solving-optimization-problem">3.3. Solving Optimization Problem</h3>
<p>There are two variable in equation \eqref{eq:5} which are center of each group of observation and label vector of each observation. The problem could be solved by fixed each variable then minimize the other variable.
<a name="331-fixed-%5Cmathbfm-center-of-observation-group"></a></p>
<h4 id="331-fixed-mathbfm-center-of-observation-group">3.3.1. Fixed <script type="math/tex">\mathbf{M}</script>, center of observation group</h4>
<p>Because all centers (<script type="math/tex">M</script>) are constant, the objective is to correctly identify label vector which is identifying the group that each observation belonged to so that the square error in equation \eqref{eq:4} is minimized.
\begin{equation} \tag{6}\label{eq:6}
y_i = argmin_{y_i}\sum_{j=1}^{K}y_{ij}||x_i-m_j||^2
\end{equation}
Retrieving from equation \eqref{eq:1}. Because only one element in vector <script type="math/tex">y_i</script> <script type="math/tex">i\in[1,K]=1</script>. Equation \eqref{eq:6} could be rewritten as:
\begin{equation} \tag{7}\label{eq:7}
j=argmin_{j}||x_i-m_j]]^2
\end{equation}
The value of <script type="math/tex">||x_i-m_j||^2</script> is the square of distance from observation to center of group in euclidean space. Concretely, when M is constant, equation \eqeqref{eq:7} shows that minimizing the sum of square error could be achieved by choosing label vector so that the center are closest to observation.
<a name="332-fixed-%5Cmathbfy-label-vector-of-each-observation"></a></p>
<h4 id="332-fixed-mathbfy-label-vector-of-each-observation">3.3.2. Fixed <script type="math/tex">\mathbf{Y}</script>, label vector of each observation</h4>
<p>When label vector (<script type="math/tex">Y</script>) are constant, the objective is to correctly identify the center so that the square error in equation \eqref{eq:4} is minimized. In this case, the optimization problem in equation \eqref{eq:5} could be rewritten by the following equation.
\begin{equation} \tag{8}\label{eq:8}
m_j=argmin_{m_j}\sum_{i=1}^{N}y_{ij}||x_i-m_j||^2
\end{equation}
The equation \eqref{eq:8} is a convex function and differentiable for each <script type="math/tex">i\in[1,N]</script>. Hence equation \eqref{eq:8} could be solved by finding the root of the partial derivative function. This approach will make sure that the root is the the value that make the function reach a optimum.
Let’s <script type="math/tex">g(m_j)=\sum_{i=1}^{N}y_{ij}||x_i-m_j||^2</script> (retrieving from equation \eqref{eq:8} and take the partial derivative of <script type="math/tex">g(m_j)</script>:
\begin{equation} \tag{9}\label{eq:9}
\frac{\partial g(m_j)}{\partial m_j}=2 \sum_{i=1}^{N}y_{ij}(m_j-x_i)
\end{equation}
The equation \eqref{eq:9} is equal 0 is equivalent to:
\begin{equation} \tag{10}\label{eq:10}
m_j\sum_{i=1}^{N}y_{ij}=\sum_{i=1}^{N}y_{ij}x_{i} 
\end{equation}
\begin{equation} \tag{11}\label{eq:11}
\Leftrightarrow m_j=\frac{\sum_{i=1}^{N}y_{ij}x_{i}}{\sum_{i=1}^{N}y_{ij}}
\end{equation}
The value of <script type="math/tex">y_{ij}=1</script> when observation <script type="math/tex">x_i</script> belongs to group <script type="math/tex">m_j</script>. Hence, the denominator of equation \eqref{eq:11} <script type="math/tex">\sum_{i=1}^{N}y_{ij}</script> is the number of observations that belonging to group <script type="math/tex">m_j</script> and the nominator <script type="math/tex">\sum_{i=1}^{N}y_{ij}x_{i}</script> is the sum of all observations belonging to group <script type="math/tex">m_j</script>.
In other word, when Y is constant, the square errors could be minimize by assigning the centers to the means of observations in the groups that the observations belonging to.
<a name="34-algorithm-summary-and-flowchart"></a></p>
<h3 id="34-algorithm-summary-and-flowchart">3.4. Algorithm Summary and Flowchart</h3>
<p><a name="341-summary"></a></p>
<h4 id="341-summary">3.4.1. Summary</h4>
<p>The algorithm can be done by continuously constantize Y and M, one each a time as discussed in <a href="#fixed-%5C%5C%5Cmathbfm%5C%5C-center-of-observation-group">Fixed <script type="math/tex">M</script></a>and <a href="#fixed-%5C%5C%5Cmathbfy%5C%5C-label-vector-of-each-observation">Fixed <script type="math/tex">Y</script></a>.</p>
<p class="message">
Step 1. Select K points as cluster centers randomly. (K is the number of group/cluster)<br />
Step 2.	Assign all data points to corresponding cluster center created in step 1 based on Euculidean distance.<br />
Step 3.	Move cluster center, new cluster centers are means of coordination of all data points among same group.<br />
Step 4. Repeat step 2 until convergence (when all cluster centers remain unchanged after an iteration).<br />
Step 5.	Clustering process is done, if we have new data points, caculate distance between that data point to the cluster centers that we found and see what group the data point belongs to.
</p>
<p><a name="342-flowchart"></a></p>
<h4 id="342-flowchart">3.4.2. Flowchart</h4>
<p>The following chart describe K-Means Algorithm.</p>
<div class="imgcap">
	<img style="display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/fig3.PNG" width="500" align="center" />
	<div class="thecap">Fig 2: K-Means Algorithm Flowchart<br /></div>
</div>
<p><a name="35-discussion"></a></p>
<h3 id="35-discussion">3.5. Discussion</h3>
<p><a name="351-convergence"></a></p>
<h4 id="351-convergence">3.5.1. Convergence</h4>
<p>The algorithm will stop after a certain number of iteration because the square error function is a strictly decreasing sequence and the square error is always greater than 0. But this algorithm will not make sure that it will find a global optimum because solving the equation \eqref{eq:8} by finding the root when the partial derivative is equal 0 will only return the value for local optima but not make sure that local optima will be a global minimum.
The following figure describe a case where poorly seeding leads to a local optimum.</p>
<div class="imgcap">
	<img style="display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/fig4.PNG" width="500" align="center" />
	<div class="thecap">Fig 3: Poorly seeded leads to inaccurate result<br /></div>
</div>
<p>This result leads to a very high square error compared to the result in <a href="#fig1">fig 1</a>.
<a name="352-sensitiveness-to-initial-cluster"></a></p>
<h4 id="352-sensitiveness-to-initial-cluster">3.5.2. Sensitiveness to initial cluster</h4>
<p>K-Means algorithm requires careful seeding, which means the final result is very sensitive to the initial value of cluster. Numerous efforts have been made to improving K-Means clustering algorithm due to its drawbacks <a href="#ref2"><script type="math/tex">^[2]</script></a>.
<a name="4-application-in-data-compression"></a></p>
<h2 id="4-application-in-data-compression">4. Application in Data Compression</h2>
<div class="imgcap">
<img style="float:left; display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/compress1.png" width="500" align="center" />
<img id="fig1" style="float:left; display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/compress2.png" width="500" align="center" />
<div class="thecap">Left image: 900 KB, Right image (K=16 color): 70KB<br /></div>
More detail will be described in the upcomming blog <a href="https://dunglai.github.io/">Image Compression</a>
</div>
<div style="clear:left;"></div>

<p><a name="5-conclusion"></a></p>
<h2 id="5-conclusion">5. Conclusion</h2>
<p>K-Means Algorithm could be very simple and quick to be implemented, the clustering problems where all clusters are centroids and separated can be solved by the algorithms.</p>

<p>This report doesn’t come with new idea to improve the effectiveness of the algorithm, the aim of the report is to introduce the readers to a basic clustering method with some visual examples on 2-dimensional and 3-dimensional data.
<a name="6-acknowledgement"></a></p>
<h2 id="6-acknowledgement">6. Acknowledgement</h2>
<p>The ideal of K-means Algorithm is leaned by me from <a href="https://www.coursera.org/learn/machine-learning">Machine Learning course</a> by <a href="http://www.andrewng.org/">Prof. Andrew Ng</a> from <a href="https://www.coursera.org">coursera</a>.</p>

<p>The mathematical approach to solve the <a href="#lost-function-and-optimization-problem">optimization problem</a> is learned by me from the article on K-means algorithm (<a href="https://machinelearningcoban.com/2017/01/01/kmeans/">machine learning fundamental</a>) by <a href="http://www.personal.psu.edu/thv102/">Tiep Vu</a>.</p>

<p><a name="61-reference"></a></p>
<h3 id="61-reference">6.1. Reference</h3>
<div id="ref1"></div>
<p>[1]. Joaqun Prez Ortega, Ma. Del, Roco Boone Rojas, and Mara J.Somodevilla <strong><em>“Research issues on k-means algorithm: An experimental trial using matlab”</em></strong>.</p>
<div id="ref2"></div>
<p>[2]. Arthur,~D.,~Vassilvitskii,~S., 2016. <strong><em>k-Means++: The Advantages of Careful Seeding</em></strong>, Technical Report, Stanford.</p>

<p><a name="7-source-code-and-pdf-report"></a></p>
<h2 id="7-source-code-and-pdf-report">7. Source Code and PDF report</h2>
<p>This part will guide you through all the code in this program, if you understand the algorithm, you could implement it on any programming language not just pascal. 
With a click of a button, all characters are separated using K-means algorithm, the program will look like this:</p>
<div class="imgcap">
<img style="float:left; display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/kmean.PNG" width="500" align="center" />
<img id="fig1" style="float:left; display: inline-block; width: 50%;" src="/public/post-assets/Kmeans/kmean1.PNG" width="500" align="center" />
<div class="thecap">Program interface<br /></div>
</div>
<div style="clear:left;"></div>

<p><a name="71-declare-data-types"></a></p>
<h3 id="71-declare-data-types">7.1. Declare Data types</h3>
<div class="language-pascal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span>
  <span class="n">POINT_RADIUS</span> <span class="p">=</span> <span class="m">5</span><span class="p">;</span>
  <span class="n">X_DIFF</span> <span class="p">=</span> <span class="m">20</span><span class="p">;</span> <span class="c1">//location panel and point location
</span>  <span class="n">Y_DIFF</span> <span class="p">=</span> <span class="m">160</span><span class="p">;</span> <span class="c1">//location panel and point location
</span>
<span class="k">type</span>    
  <span class="c1">// type used for application  
</span>  <span class="n">RGB</span> <span class="p">=</span> <span class="k">array</span><span class="p">[</span><span class="m">0..2</span><span class="p">]</span> <span class="k">of</span> <span class="kt">Single</span><span class="p">;</span> <span class="c1">// x-axis  // store red, green and blue of an image
</span>  <span class="n">CollumnPixels</span> <span class="p">=</span> <span class="k">array</span> <span class="k">of</span> <span class="n">RGB</span><span class="p">;</span> <span class="c1">// y-axis, rowPixels
</span>  <span class="n">imgArray</span> <span class="p">=</span> <span class="k">array</span> <span class="k">of</span> <span class="n">CollumnPixels</span><span class="p">;</span> <span class="c1">// 3-dimensional array used to store red, green, blue color of each pixel in an image 
</span>  <span class="n">IntArray</span> <span class="p">=</span> <span class="k">array</span> <span class="k">of</span> <span class="kt">Integer</span><span class="p">;</span> <span class="c1">// label
</span>
  <span class="n">SingleColorArray</span> <span class="p">=</span> <span class="k">array</span> <span class="k">of</span> <span class="n">Color</span><span class="p">;</span> <span class="c1">// 1 dimensional array used to store color
</span>  <span class="n">ColorArray</span> <span class="p">=</span> <span class="k">array</span> <span class="k">of</span> <span class="n">SingleColorArray</span><span class="p">;</span> <span class="c1">// 2 dimensional array used to store color
</span>  
  <span class="c1">// typed used for Visualization
</span>  <span class="n">PointsRecord</span> <span class="p">=</span> <span class="k">Record</span>
    <span class="n">x</span>    <span class="p">:</span> <span class="kt">Single</span><span class="p">;</span> <span class="c1">// x-axit on panel
</span>    <span class="n">y</span>    <span class="p">:</span> <span class="kt">Single</span><span class="p">;</span> <span class="c1">// y-axit on panel
</span>    <span class="n">idx</span>  <span class="p">:</span> <span class="kt">Integer</span><span class="p">;</span> <span class="c1">// store label of each point, instead of create another array or enumeration to store it
</span>  <span class="k">end</span><span class="p">;</span>

  <span class="n">PointsArray</span> <span class="p">=</span> <span class="k">array</span> <span class="k">of</span> <span class="n">PointsRecord</span><span class="p">;</span> <span class="c1">// Store all the points on panel and the init points
</span></code></pre></div></div>
<p>In the panel, each point’s data type is <code class="highlighter-rouge">PointsRecord</code>, it has <code class="highlighter-rouge">x</code> and <code class="highlighter-rouge">y</code> properties, they are the x-axis and y-axis of each point, <code class="highlighter-rouge">idx</code> is the label of each point, <code class="highlighter-rouge">idx</code> keep changing overtime during the iterations.</p>

<p>Continue Reading… Please download source code!</p>

<p><a href="/public/post-assets/Kmeans/Report.pdf">Download PDF Report</a>
Full code in Pascal: https://github.com/DungLai/Image-Compression-Segmentation</p>

</div>
        <!-- Start of StatCounter Code for Default Guide -->
        <hr>
Total visits:
<script type="text/javascript">
var sc_project=11553218; 
var sc_invisible=0; 
var sc_security="55ea133f"; 
var sc_text=2; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics
Made Easy - StatCounter" href="http://statcounter.com/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11553218/0/55ea133f/0/" alt="Web
Analytics Made Easy - StatCounter"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
<a href="http://statcounter.com/p11553218/?guest=1">(Powered by Statcounter)</a>
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/01/25/SVD/">
            Singular Value Decomposition
            <small>25 Jan 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/12/21/gradient-descent/">
            Gradient Descent
            <small>21 Dec 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/12/09/k-nearest-neighbors/">
            K Nearest Neighbors
            <small>09 Dec 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/10/10/linear-regression/">
            Linear Regression
            <small>10 Oct 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/09/21/FlappyBirdAI/">
            AI for FlappyBird Game
            <small>21 Sep 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://dunglai-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
   
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>

  
</html>
