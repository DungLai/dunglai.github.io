<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Journey Preparation Tool Project &middot; Dung Lai
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110006044-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110006044-1');
  </script> 

  <!-- toogle button -->
  <script>
  function myFunction() {
      var x = document.getElementById("myDIV");
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
  }
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A personal website of <a href="https://dunglai.github.io/about" target="">Dung Lai</a>, containing technical blog on technology.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    
      
    
      
        
      
    
      
        
      
    
      
    
    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2018. All rights reserved.
    </p>
  </div>
</div>

 
    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Dung Lai</a>
            <small>blog</small>             
          </h3>
        </div>
      </div>
      
      <link rel="stylesheet" href="/public/css/blog-index.css">

<nav>
	<div class='blog-wrapper'>
		<div class='blog-index'>
			<strong><u>Machine learning</u></strong></br>
				<li><a href='https://dunglai.github.io/2018/01/25/SVD/'>7. Singular Value Decomposition</a></li>
				<li><a href='https://dunglai.github.io/2017/12/21/gradient-descent/'>6. Gradient Descent</a></li>
				<li><a href='https://dunglai.github.io/2017/12/09/k-nearest-neighbors/'>5. K Nearest Neighbors</a></li>
				<li><a href='https://dunglai.github.io/2017/10/10/linear-regression/'>4. Linear Regression</a></li>
				<li><a href='https://dunglai.github.io/2017/09/21/FlappyBirdAI/'>3. AI for flappy bird game (Neural Network, Genetic Algorithm)</a></li>
				<li><a href='https://dunglai.github.io/2017/06/10/image-compression/'>2. Image Segmentation using K-Means</a></li>
				<li><a href='https://dunglai.github.io/2017/06/01/k-means/'>1. K-Means Clustering</a></li>
			<strong><u>Other</u></strong></br>
				<li><a href='https://dunglai.github.io/2017/06/30/mongodbmapping/'>4. Mapping MySQL to MongoDB</a></li>
				<li><a href='https://dunglai.github.io/2017/06/27/UniversityMySQL/'>3. University MySQL DB</a></li>
				<li><a href='https://dunglai.github.io/2017/05/27/Planetary-Rover/'>2. Planetary Rover Game (C#, Winform)</a></li>
				<li><a href='https://dunglai.github.io/2017/03/01/Music-Player/'>1. Music Player (Pascal)</a></li>
			<strong><u>Project</u></strong></br>
				<li><a href='https://dunglai.github.io/2018/04/09/NavigationTrainer/'>1. Journey Preparation Tool</a></li>
		</div>
	</div>
</nav>

      <div class="container content">
        <div class="post">
  <h1 style="font-size: 130%;" class="post-title">Journey Preparation Tool Project</h1>
  <span class="post-date" style="float:none;">09 Apr 2018</span>
  
  <h3><a href="https://dunglai.github.io/COM/index.html"><strong>Demo web application</strong></a></h3>

<div class="imgcap">
<img style="display: inline-block; width: 100%;" src="/public/post-assets/NavigationTrainer/poster.PNG" width="500" align="center" />
<div class="thecap">Research Poster</div>
</div>

<p><strong>Content:</strong></p>

<!-- MarkdownTOC -->

<ul>
  <li><a href="#0">Abstract</a></li>
  <li><a href="#1">1. Introduction</a></li>
  <li><a href="#2">2. Surround Sound Technology</a>
    <ul>
      <li><a href="#2.1">2.1. Ambisonic Technology</a></li>
      <li><a href="#2.2">2.2. Recording Technique</a></li>
    </ul>
  </li>
  <li><a href="#3">3. Development</a>
    <ul>
      <li><a href="#3.1">3.1. Web Application, voice-based interaction</a></li>
      <li><a href="#3.2">3.2. Desktop Application, external Headtracker integrated on headphone</a></li>
      <li><a href="#3.3">3.3. Mobile Application, rotation capability using on device sensor</a></li>
    </ul>
  </li>
  <li><a href="#4">4. User Experience Testing</a></li>
  <li><a href="#5">5. Technical Challanges</a></li>
  <li><a href="#6">6. Acknowledgement</a></li>
  <li><a href="#7">7. Reference</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="0"></a></p>
<h2 id="abstract">Abstract</h2>

<p>This report summarizes the development of a long-term scalable technology-enable solution to assist journey preparation for members of vision impaired community. The existing application allows users to familiarize themselves with a specific location in the CBD by letting them experience the immersive auditory-based simulator supported with state-of-the-art surround-sound technology called Ambisonic and dynamic interactive voice interface.</p>

<p><a name="1"></a></p>
<h2 id="1-introduction">1. Introduction</h2>

<p>With almost one in five Australians experiencing some form of disability, a substantial proportion of the community faces challenges to actively participate in city life [1]. This project is developing a long-term scalable technology-enable solution to assist journey preparation for members of vision impaired community. A cross-platform application has been developed to simulate the sensory experience of Flinders Street Railway Station located in the Melbourne CBD. The simulator would allow users to rotate their body and experience the sounds change.</p>

<p><a name="2"></a></p>
<h2 id="2-surround-sound-technology">2. Surround Sound Technology</h2>

<p><a name="2.1"></a></p>
<h3 id="21-ambisonic-technology">2.1. Ambisonic Technology</h3>

<p>To achieve an “immersive” experience, Ambisonic Technology [2]: a full-sphere surround sound technique, in addition to the horizontal plane, it covers sound sources above and below the listener. 3D audio can be reproduced over many loudspeakers positioned all around the listener. For the scope of the project, the 3D audio is played through the headphone, the technology used is called Binaural Synthesis. It consists of simulating, at someone’s eardrums, the same acoustical sound field produced as one which would have been produced by a real audio scene. This immersive 3D audio will assist people with vision impairment to visualize the surrounding areas in conjunction with improving their mental mapping skills.</p>

<div class="imgcap">
<img style="display: inline-block; width: 40%;" src="/public/post-assets/NavigationTrainer/fig1.png" width="500" align="center" />
<div class="thecap">Fig. 1. Stereo sound and 3D audio comparison [3]</div>
</div>

<p><a name="2.2"></a></p>
<h3 id="22-recording-technique">2.2. Recording Technique</h3>

<p>Sounds are recorded by a tetrahedron microphone, which contains 4 capsules pointing toward 4 different directions (fig 2.1), to get 4-channel monophonic A-format. This is converted into 4-channel B-format, encoding the directional information of a given three-dimensional sound field to four channels called W, X, Y, Z using mathematical formula (fig 2.2)</p>

<div class="imgcap" style="text-align: center;">
<img style="display: inline-block; width: 30%;" src="/public/post-assets/NavigationTrainer/mic.png" width="500" align="center" />
<img id="fig1" style="display: inline-block; width: 30%;" src="/public/post-assets/NavigationTrainer/math.png" width="500" align="center" />
</div>
<div style="clear:left"></div>
<div class="thecap">Fig. 2. (1) soundfield microphone , (2) B-format encoding formula [2]</div>

<p>Where <script type="math/tex">s_i</script> are mono audio signal recorded by each individual capsule we want to encode at the position <script type="math/tex">\phi</script> (horizontal angle (azimuth) phi), and <script type="math/tex">\theta</script> (vertical angle (elevation) theta) [2].
The microphone we utilized is called Sennheiser Ambeo VR Mic, an application is provided with the Mic with the capability of converting sound file from A to B. This software works as a VST2 or VST3 plugin, which can be used in any digital audio workstation (DAW) for editing purposes. Reaper software (a complete digital audio production application) is used within the project to generate a set of B-format audio files.</p>

<div class="imgcap">
<img style="display: inline-block; width: 55%;" src="/public/post-assets/NavigationTrainer/fig3.png" width="500" align="center" />
<div class="thecap">Fig. 1. Stereo sound and 3D audio comparison [3]</div>
</div>

<p><a name="3"></a></p>
<h2 id="3-development">3. Development</h2>

<p><a name="3.1"></a></p>
<h3 id="31-web-application-voice-based-interaction">3.1. Web Application, voice-based interaction</h3>

<p>The Ambisonic sound capsules have been encoded in 360 videos, recorded by Garmin Virb 360 Video Camera, using Facebook 360 Encoder (image on the right) and deployed on YouTube. To upload 360 videos with 3D audio on YouTube, the audio must be a 4 channels B-format with first-order ambiX, ACN ordering, and SN3D normalization. This will make the video compatible with VR mode, allowing users to use Google Cardboard or Oculus Rift to navigate and change direction. The video can also be rotated by sliding the video using a mouse or using WASD keys on the keyboard or using the controller on the top left of the video</p>

<div class="imgcap">
<img style="display: inline-block; width: 30%;" src="/public/post-assets/NavigationTrainer/encoder.png" width="500" align="center" />
<div class="thecap">Fig: Facebook 360 Encoder</div>
</div>

<p>Users can interact with the interface using either voice or text commands and then narration will be played back upon user’s request. This function is based on <a href="https://www.google.com/chrome/demos/speech.html">Web Speech API</a> [6], a JavaScript API provided by Google Inc to enable web developers to incorporate speech recognition and synthesis into web pages. Here are current support commands that users can interact with the web interface via microphone.</p>

<ul>
  <li>Help – provide a list of available commands for users</li>
  <li>What is this – describe the project and website</li>
  <li>Where am I – describe the place where the video was recorded</li>
  <li>Describe – describe the surrounding environment</li>
  <li>When recorded – show the time when the video was recorded</li>
  <li>Next location – play the next video</li>
  <li>Replay – replay the current video</li>
  <li>Hi/ Hello – guide user to help command.</li>
  <li>Start Exploring – play the first location (outside Flinders Street Station entrance)</li>
</ul>

<div class="imgcap">
<img style="display: inline-block; width: 80%;" src="/public/post-assets/NavigationTrainer/web_interface.png" width="500" align="center" />
<div class="thecap">Fig. 4. Web application interface </div> Link to Web Demo: https://dunglai.github.io/COM/index.html
</div>

<p><a name="3.2"></a></p>
<h3 id="32-desktop-application-external-headtracker-integrated-on-headphone">3.2. Desktop Application, external Headtracker integrated on headphone</h3>

<p>A Desktop application is also being developed using Max/MSP, a visual programming language, to explore the use of headtracking, allowing the tracking of yaw-pitch-roll value that determine the orientation of the head/body. This value is used to change sound field and generate directional narration.</p>

<div class="imgcap">
<img style="display: inline-block; width: 25%;" src="/public/post-assets/NavigationTrainer/rotation.png" width="500" align="center" />
<div class="thecap">Fig: 3 values that define the orientation in 3D space</div>
</div>

<p>The Hedrot head tracker [9] is successfully implemented and the integration is shown in figure 4. By separate the Yaw value from the head tracker into 4 quarters representing front, left, right and back, the surround area will be described so that description is on-request and dynamically adjusted based on head direction.</p>

<div class="imgcap">
<img style="display: inline-block; width: 55%;" src="/public/post-assets/NavigationTrainer/fig5.png" width="500" align="center" />
<div class="thecap">Fig. 5. Teensy board parts – Schematics – Build head tracker – Integration on the headphone</div>
</div>

<p>The implementation of Hedrot headtracker and desktop application is done in Max/MSP programming language.</p>

<div class="imgcap">
<img style="display: inline-block; width: 55%;" src="/public/post-assets/NavigationTrainer/maxmsp_interface.png" width="500" align="center" />
<div class="thecap">Fig. 6. Desktop application interface </div>
</div>

<p><a name="3.3"></a></p>
<h3 id="33-mobile-application-rotation-capability-using-on-device-sensor">3.3. Mobile Application, rotation capability using on device sensor</h3>

<p>In the process of building a proof of concept in a variety of platforms, an Android application was developed, allowing users to experience immersive spatial audio recorded in various locations on Flinders Street Station. The application is capable of changing soundfield using hardware-based sensors which are physical components built into a handset or tablet device. Specifically, data is extracted from the geomagnetic field sensor in combination with the accelerometer using android sensor API [8] to determine a device’s position relative to the magnetic north pole, this orientation data in quaternion format is then passed through Google VR audio engine to rotate Ambisonic soundfield.</p>

<p>The <a href="https://developers.google.com/vr/reference/android/com/google/vr/sdk/audio/GvrAudioEngine">Google VR audio engine</a> [7] allows the user to spatialize sound sources in 3D space, including distance and height cues. The GvrAudioEngine is capable of playing back spatial sound in two separate ways: Sound Object rendering, allows the user to create a virtual sound source in 3D space, the second method allows the user to play back Ambisonic soundfield. The first method was used to create the artificial directional audio narration of surrounding environment and objects, the second method was used to playback Ambisonic audio files recorded in real-time using Sennheiser Ambeo VR Microphone.</p>

<div class="imgcap">
<img style="display: inline-block; width: 40%;" src="/public/post-assets/NavigationTrainer/mobile_interface.png" width="500" align="center" />
<div class="thecap">Fig. 7. Android application interface </div>
</div>

<p><a href="https://github.com/DungLai/Navigation-Trainer">Github link to Java code</a></p>

<p>Get Jaw-Pitch-Roll from android sensor:</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">MainActivity</span> <span class="kd">extends</span> <span class="n">Activity</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kt">float</span><span class="o">[]</span> <span class="n">mOrientation</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">float</span><span class="o">[]</span> <span class="n">mQuaternion</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">float</span><span class="o">[]</span> <span class="n">mMagneticField</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">float</span><span class="o">[]</span> <span class="n">mGravity</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">float</span> <span class="n">mAzimut</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">float</span> <span class="n">mPitch</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">float</span> <span class="n">mRoll</span><span class="o">;</span>
    <span class="n">SensorManager</span> <span class="n">sm</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
    <span class="n">TextView</span> <span class="n">textView1</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
    <span class="n">TextView</span> <span class="n">testView</span><span class="o">;</span>
    <span class="n">List</span> <span class="n">list</span><span class="o">;</span>

    <span class="n">SensorEventListener</span> <span class="n">sel</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SensorEventListener</span><span class="o">(){</span>
        <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onAccuracyChanged</span><span class="o">(</span><span class="n">Sensor</span> <span class="n">sensor</span><span class="o">,</span> <span class="kt">int</span> <span class="n">accuracy</span><span class="o">)</span> <span class="o">{}</span>
        <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onSensorChanged</span><span class="o">(</span><span class="n">SensorEvent</span> <span class="n">event</span><span class="o">)</span> <span class="o">{</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">event</span><span class="o">.</span><span class="na">sensor</span><span class="o">.</span><span class="na">getType</span><span class="o">()</span> <span class="o">==</span> <span class="n">Sensor</span><span class="o">.</span><span class="na">TYPE_MAGNETIC_FIELD</span><span class="o">)</span>
                <span class="n">mMagneticField</span> <span class="o">=</span> <span class="n">event</span><span class="o">.</span><span class="na">values</span><span class="o">;</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">event</span><span class="o">.</span><span class="na">sensor</span><span class="o">.</span><span class="na">getType</span><span class="o">()</span> <span class="o">==</span> <span class="n">Sensor</span><span class="o">.</span><span class="na">TYPE_ACCELEROMETER</span><span class="o">)</span>
                <span class="n">mGravity</span> <span class="o">=</span> <span class="n">event</span><span class="o">.</span><span class="na">values</span><span class="o">;</span>
            <span class="k">if</span> <span class="o">((</span><span class="n">mGravity</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">||</span> <span class="o">(</span><span class="n">mMagneticField</span> <span class="o">==</span> <span class="kc">null</span><span class="o">))</span> <span class="o">{</span>
                <span class="n">textView1</span><span class="o">.</span><span class="na">setText</span><span class="o">(</span><span class="s">"null"</span><span class="o">);</span>
                <span class="k">return</span><span class="o">;</span>
            <span class="o">}</span>

            <span class="kt">float</span> <span class="n">R</span><span class="o">[]</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">float</span><span class="o">[</span><span class="mi">9</span><span class="o">];</span>
            <span class="kt">float</span> <span class="n">I</span><span class="o">[]</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">float</span><span class="o">[</span><span class="mi">9</span><span class="o">];</span>
            <span class="kt">boolean</span> <span class="n">success</span> <span class="o">=</span> <span class="n">SensorManager</span><span class="o">.</span><span class="na">getRotationMatrix</span><span class="o">(</span><span class="n">R</span><span class="o">,</span> <span class="n">I</span><span class="o">,</span> <span class="n">mGravity</span><span class="o">,</span> <span class="n">mMagneticField</span><span class="o">);</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">success</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">mOrientation</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">float</span><span class="o">[</span><span class="mi">3</span><span class="o">];</span>
                <span class="n">mQuaternion</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">float</span><span class="o">[</span><span class="mi">4</span><span class="o">];</span>
                <span class="n">SensorManager</span><span class="o">.</span><span class="na">getOrientation</span><span class="o">(</span><span class="n">R</span><span class="o">,</span> <span class="n">mOrientation</span><span class="o">);</span>
                <span class="n">mAzimut</span> <span class="o">=</span> <span class="n">mOrientation</span><span class="o">[</span><span class="mi">0</span><span class="o">];</span> <span class="c1">// orientation contains: azimut, pitch and roll</span>
                <span class="n">mPitch</span> <span class="o">=</span> <span class="n">mOrientation</span><span class="o">[</span><span class="mi">1</span><span class="o">];</span>
                <span class="n">mRoll</span> <span class="o">=</span> <span class="n">mOrientation</span><span class="o">[</span><span class="mi">2</span><span class="o">];</span>

                <span class="n">SensorManager</span><span class="o">.</span><span class="na">getQuaternionFromVector</span><span class="o">(</span><span class="n">mQuaternion</span><span class="o">,</span> <span class="n">mOrientation</span><span class="o">);</span>
            <span class="o">}</span>
            <span class="n">gvrAudioEngine</span><span class="o">.</span><span class="na">setSoundfieldRotation</span><span class="o">(</span><span class="n">soundFieldId</span><span class="o">,</span> <span class="n">mQuaternion</span><span class="o">[</span><span class="mi">0</span><span class="o">],</span> <span class="n">mQuaternion</span><span class="o">[</span><span class="mi">1</span><span class="o">],</span> <span class="n">mQuaternion</span><span class="o">[</span><span class="mi">2</span><span class="o">],</span> <span class="n">mQuaternion</span><span class="o">[</span><span class="mi">3</span><span class="o">]);</span>

            <span class="c1">// print rotation value to screen</span>
            <span class="n">textView1</span><span class="o">.</span><span class="na">setText</span><span class="o">(</span><span class="s">"x: "</span><span class="o">+</span> <span class="n">mAzimut</span> <span class="o">+</span><span class="s">"\ny: "</span><span class="o">+</span> <span class="n">mPitch</span> <span class="o">+</span><span class="s">"\nz: "</span><span class="o">+</span> <span class="n">mRoll</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">};</span>
 <span class="o">}</span>
</code></pre></div></div>

<p>Load soundfield using GoogleVR audio engine and pass data from sensor:</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">MainActivity</span> <span class="kd">extends</span> <span class="n">Activity</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="n">GvrAudioEngine</span> <span class="n">gvrAudioEngine</span><span class="o">;</span>
<span class="c1">//    private volatile int sourceId = GvrAudioEngine.INVALID_ID;</span>
    <span class="kd">private</span> <span class="kd">volatile</span> <span class="kt">int</span> <span class="n">soundFieldId</span> <span class="o">=</span> <span class="n">GvrAudioEngine</span><span class="o">.</span><span class="na">INVALID_ID</span><span class="o">;</span>


        <span class="c1">// Initialize 3D audio engine.</span>
        <span class="n">gvrAudioEngine</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GvrAudioEngine</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">GvrAudioEngine</span><span class="o">.</span><span class="na">RenderingMode</span><span class="o">.</span><span class="na">BINAURAL_LOW_QUALITY</span><span class="o">);</span>

        <span class="n">radbut2</span><span class="o">.</span><span class="na">setOnClickListener</span><span class="o">(</span><span class="k">new</span> <span class="n">View</span><span class="o">.</span><span class="na">OnClickListener</span><span class="o">()</span> <span class="o">{</span>
            <span class="nd">@Override</span>
            <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onClick</span><span class="o">(</span><span class="n">View</span> <span class="n">v</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">gvrAudioEngine</span><span class="o">.</span><span class="na">stopSound</span><span class="o">(</span><span class="n">soundFieldId</span><span class="o">);</span>
                <span class="n">soundFieldId</span> <span class="o">=</span> <span class="n">gvrAudioEngine</span><span class="o">.</span><span class="na">createSoundfield</span><span class="o">(</span><span class="s">"2-Bformat-32bits.wav"</span><span class="o">);</span>
                <span class="n">gvrAudioEngine</span><span class="o">.</span><span class="na">setSoundVolume</span><span class="o">(</span><span class="n">soundFieldId</span><span class="o">,</span> <span class="mi">40</span><span class="o">);</span>
                <span class="n">gvrAudioEngine</span><span class="o">.</span><span class="na">playSound</span><span class="o">(</span><span class="n">soundFieldId</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
                <span class="n">testView</span><span class="o">.</span><span class="na">setText</span><span class="o">(</span><span class="s">"Currently playing: Location 2"</span><span class="o">);</span>
            <span class="o">}</span>
        <span class="o">});</span>

    <span class="nd">@Override</span>
    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">onStop</span><span class="o">()</span> <span class="o">{</span>
        <span class="k">if</span><span class="o">(</span><span class="n">list</span><span class="o">.</span><span class="na">size</span><span class="o">()&gt;</span><span class="mi">0</span><span class="o">){</span>
            <span class="n">sm</span><span class="o">.</span><span class="na">unregisterListener</span><span class="o">(</span><span class="n">sel</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="kd">super</span><span class="o">.</span><span class="na">onStop</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p><a name="4"></a></p>
<h2 id="4-user-experience-testing">4. User Experience Testing</h2>

<p>The information being delivered by the app is refined constantly during the project to ensure that users will be benefited by the given information. A small consultation has been made with <a href="http://www.lildeverell.net/">Lil Deverell</a> – an orientation and mobility specialist with the aim of improving the quality of the overall system. It is suggested to add doppler effect in the platform, showing when and where trains are going, the transition between locations, the sound of cane, GPS sequence instructions in linear order. These factors will help users develop mental mapping skills. Furthermore, videos should be captured at different time of day and ideally, in crowded and pressure environment, crossroad route should also be recorded. This will prepare them with the real pressure of navigating around the city on their own and reduce their stress.</p>

<p>Improvement could also be done to existing directional description. The information provided need to have learned elements. The users should be notified when their orientation is perpendicular to the train lines or aligned with the train direction. Information such as tactile pavers and objects they can hear in real-time is also beneficial in making educated guesses. Information is not restricted to visual, non-visual cue such as smells, materials, temperature, wind, light pulse (for people with low vision), lift, entrance point, coffee shop sounds, barrier, reflect sound, consistent sound, and toilet. Additionally, information about the structure of the area will help users to get the big picture. Another significant detail that help reducing stress is preparing them to find convenient spots to wait for someone, this could be corners, spots where low vision people can lean on, solid objects that they can touch, and balance surface, this prevents them from standing on the way of other people. Lastly, the source of assistance is also as important as other factors.</p>

<p><a name="5"></a></p>
<h2 id="5-technical-challanges">5. Technical Challanges</h2>

<p>Currently, the existing feature of the system has many technical aspects that need to be investigated. The Ambisonic playback on mobile application produces some form of white noise when the sound fields are being changed, audio processing techniques can be applied to reduce the level of white noise (the mixture of different frequency). Voice recognition and gesture recognition are useful features of the mobile application, it helps people with vision impairment interact with the application without other sources of assistance. Each Ambisonic B-format file contains 4 channels with high fidelity sound quality, this leads to the increase in memory if files are encoded in the Android app itself. However, the Android application has limited heap space, other approaches such as uploading sound files to the cloud and let users download them individually can solve the problem. Currently, users must put their Android device horizontally to correctly rotate the sound field, this is not effective and unusual for users. The application should allow users to point the device toward the directions that they want to rotate into.</p>

<p><a name="6"></a></p>
<h2 id="6-acknowledgement">6. Acknowledgement</h2>

<p>The authors would like to thank Stuart Favilla, David Sly, Lil Deverell, Denny Myer and other faculty members of Swinburne University as well as other summer internship students for providing insight and expertise and equipment that greatly assisted the research and development.</p>

<p><a name="7"></a></p>
<h2 id="7-reference">7. Reference</h2>

<p>[1]	Open Innovation Competition on Accessibility [http://www.melbourne.vic.gov.au/about-melbourne/melbourne-profile/smart-city/Pages/innovation-competition-city-accessibility.aspx]</p>

<p>[2]	An Introduction to Higher Order Ambisonic by Florian Hollerweger, 2008 [http://flo.mur.at/writings/HOA-intro.pdf]</p>

<p>[3]	3D Audio, 3D sound lab [http://pro.3dsoundlabs.com/category/intro-to-3d-audio/]</p>

<p>[4]	Ambeo software for virtual Reality [https://en-us.sennheiser.com/ambeo-blueprints-virtual-reality]</p>

<p>[5]	Add Spatial Audio to a Video For YouTube [https://fb360spatialworkstation.zendesk.com/hc/en-us/articles/207888189-Add-Spatial-Audio-to-a-Video-For-YouTube]</p>

<p>[6]	Web Speech API [https://w3c.github.io/speech-api/speechapi.html]</p>

<p>[7] 	Google VR Audio Engine for Android [https://developers.google.com/vr/reference/android/com/google/vr/sdk/audio/GvrAudioEngine]</p>

<p>[8]  Android Sensor API [https://developer.android.com/guide/topics/sensors/sensors_overview.html]</p>

<p>[9]  Hedrot headtracker [https://abaskind.github.io/hedrot/]</p>

<hr />

<p><a href="/public/post-assets/NavigationTrainer/TechnicalReport.pdf"><strong>Download Technical Report in PDF</strong></a></p>

<p><a href="/public/post-assets/NavigationTrainer/poster.pdf"><strong>Download Poster</strong></a></p>

</div>
        <!-- Start of StatCounter Code for Default Guide -->
        <hr>
Total visits:
<script type="text/javascript">
var sc_project=11553218; 
var sc_invisible=0; 
var sc_security="55ea133f"; 
var sc_text=2; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics
Made Easy - StatCounter" href="http://statcounter.com/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11553218/0/55ea133f/0/" alt="Web
Analytics Made Easy - StatCounter"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
<a href="http://statcounter.com/p11553218/?guest=1">(Powered by Statcounter)</a>
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/07/10/cars-visualisation/">
            Interactive Visualisation
            <small>10 Jul 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/01/25/SVD/">
            Singular Value Decomposition
            <small>25 Jan 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/12/21/gradient-descent/">
            Gradient Descent
            <small>21 Dec 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/12/09/k-nearest-neighbors/">
            K Nearest Neighbors
            <small>09 Dec 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/10/10/linear-regression/">
            Linear Regression
            <small>10 Oct 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://dunglai-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
   
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>

  
</html>
