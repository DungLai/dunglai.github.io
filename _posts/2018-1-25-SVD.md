---
layout : post
title : Singular Value Decomposition
desc : <div class="tag">Linear Algebra</div><div class="tag">Dimesnionality Reduction</div><br>
img  : ../public/post-assets/SVD/matrix-factorization.png
---

**Content:**
<!-- MarkdownTOC depth=4 -->

- [Diagonalize](#diagonalize)
- [SVD](#svd)
- [Image Compression using SVD](#image-compression-using-svd)
	- [Energy based](#energy-based)
	- [Evaluation](#evaluation)
	- [Python Code](#python-code)
- [Reference](#reference)
- [SVD](#svd-1)

<!-- /MarkdownTOC -->

<a name="diagonalize"></a>
### Diagonalize
If a matrix $$A$$ can be diagonalized by matrix $$P$$:
\begin{equation}
P^-1AP = \begin{bmatrix} \tag{1}\label{eq:1}
 \lambda_1 &  &  &  \newline
  & \lambda_2 &  &  \newline
  &  & \ddots  &  \newline
  &  &  & \lambda_n
\end{bmatrix}
\end{equation}
then:

\begin{equation} \tag{2}\label{eq:2}
AP = P \begin{bmatrix}
 \lambda_1 &  &  &  \newline
  & \lambda_2 &  &  \newline
  &  & \ddots  &  \newline
  &  &  & \lambda_n
\end{bmatrix}
\end{equation}

Let's $$p_i$$ be collumn vectors of matrix $$P$$:
\begin{equation} \tag{3}\label{eq:3}
P = ( \vec{p_1} \hspace{2em} \vec{p_2} \hspace{2em} ... \hspace{2em} \vec{p_n}) 
\end{equation}

Equation \eqref{eq:2} can be rewritten as
$$A \vec{p_i} = \lambda_i \vec{p_i} \hspace{1.2em} (i=1,2,...,n)$$ 

So the column vectors of $$P$$ are right eigenvectors of $$A$$, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of $$P$$ also suggests that the eigenvectors are linearly independent. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of $$P^{-1}$$ are the left eigenvectors of $$A$$.

**A matrix is diagonalizable if P is invertible: all eigenvectors are linearly independent.**

Check for nondiagonalizable matrices:
1. Calculate geometric multiplicity (GM): The number of independent eigenvectors.
2. Calculate algebraic multiplicity (AM): The number of repeated $$\lambda$$.
3. GM < AM => nondiagonalizable

<div class='message'>Every symetric matrix is diagonalizable and their eigenvectors are orthonormal vectors</div>

This is "spectral theorem" or "principle axis theorem": For every symetric matrix A,
\begin{equation}
A = QDQ^{-1} = QDQ^T \hspace{2em} with \hspace{1em} Q^{-1} = Q^T
\end{equation}

(Proof [1] page 331)


<a name="svd"></a>
### SVD
<a name="image-compression-using-svd"></a>
### Image Compression using SVD
<a name="energy-based"></a>
#### Energy based
<a name="evaluation"></a>
#### Evaluation
<a name="python-code"></a>
#### Python Code
<a name="reference"></a>
### Reference
1. Introduction to linear algebra 4th edition, Gilbert Strang.

basis for row space : v1
basis for col space : u1

Av1 = sigma u1 (same number of basic cuz number of basis for row space = no of basis for col space = rank )
=> AV  = ZU (U and V and Z max length = r, now we need to extend it to m)

A^TA = V Z^2 V^T. A^TA is symetric so it can be diagonalizaed (spectral theorem) -> V is eigenvectors of A^TA, they are orthonormal vector. Same for U.


NOW HOW TO FILL UP THE REST OF VECTOR TO COMPLETE SVD?
IS THIS A LEGIT PROOF FOR EXISTENCE OF SVD?

<a name="svd-1"></a>
### SVD 

Suppose we know the (proved fact)[add link] that every square matrix can be factorized into the product of 3 special matrixed. 

We call v1 is a vector in row space. So Av_1 will be the linear combination of collumn vectors in A, the coefficients is elements in v_1. This combination of collumn vector in A creates a new vector in collumn space of A. -> Av1 = sigma u1, we take v1...v_r : basis of row space and we get AV=UZ (not ZU) bc number of basis for col space and row space are equal, now need to prove that U is also orthornomal providing that V is orthornormal basis of row space.

multiply both side with V^T => A = UZV^T
A^TA = VZ^T U^T UZV^T = V Z^2 V^T


Proof that U is orthogonal matrix: 
v1, v2 are orthogonal vector => v1 v2^T = 0 
u1.u2 (dot product) = Av1.v2^T A^T = 0


=>
A^T A V = VZ^2

=> A^TA v_i = z_i^2 v_i

=> v_i is eigen... (sometime it doesnt exist but A^TA is symetric positive definite matrix)
