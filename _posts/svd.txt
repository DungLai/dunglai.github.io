Singular Value Decomposition

1. Diagonalization pg298
A (nxn) with n linearly independent eigenvectors x_1,...,x_n (put them in matrix S) can be diagonalized  S^-1 A S = lambda (diagonal matrix contains eigenvalues) (pg.298) -> A = S lambda S^-1

Invertability is concerned with the eigenvalues (lambda=0 or != 0)
Diagonalizability is concerned with eigenvectors (too few or enough for S)

Good thing: Power k of A: A^k = S lambda^k S^-1

2. Eigenvectors vs Pivots

Product of pivots = det = product of eigenvalues
Sum of eigenvalues = sum entries on diagonal (trace)

The number of positive eigenvalues of A=A^T equals the number of positive pivots

3. Symetric matrix
only real eigenvalues
Eigenvectors corresponding to distinct eigenvalues are
orthogonal.
diagonalizable (spectral theorem)(pg330, 335) (WHY?)


We have seen in the previous pages and in lecture notes
that if A ∈ R
n×n
is a symmetric matrix then it has an
orthonormal set of eigenvectors u1, u2, . . . , un
corresponding to (not necessarily distinct) eigenvalues
λ1, λ2, . . . , λn, then we have:

AA^T is semi positive (entries on diag >= 0)

Corollary (he qua) : if A=A^T  then A=Q lambda Q^-1 = Q lambda Q^T with Q^-1 = Q^T.

Real eigenvalues in lambda and orthonormal eigenvectors in Q.

Proof:

Schur's Theorem: Every square matrix factors into A=QTQ^-1. T is upper triangle and 

http://control.ucsd.edu/mauricio/courses/mae280a/lecture11.pdf

AA^T symetric => eigenvector : This is used for calculation, Im still working on existence of SVD

proof of existence of svd for every real rectangle matric 
http://db.cs.duke.edu/courses/cps111/spring07/notes/12.pdf (pg.151)